from __future__ import annotations

import json
import logging
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Iterable, Mapping, Optional, Sequence, Tuple

import duckdb
import numpy as np
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq

from wepppy.all_your_base.stats import weibull_series
from wepppy.query_engine import activate_query_engine, resolve_run_context, update_catalog_entry
LOGGER = logging.getLogger(__name__)

EVENTS_REL_PATH = Path("wepp/output/interchange/return_period_events.parquet")
RANKS_REL_PATH = Path("wepp/output/interchange/return_period_event_ranks.parquet")
EBE_DATASET = "wepp/output/interchange/ebe_pw0.parquet"
TOTALWATSED3_DATASET = "wepp/output/interchange/totalwatsed3.parquet"

CLIMATE_REQUIRED = {
    "year": ("year", "Year"),
    "month": ("month", "Month"),
    "day": ("day_of_month", "Day", "day"),
    "intensity_10": ("10-min Peak Rainfall Intensity (mm/hour)", "peak_intensity_10", "i10"),
    "intensity_15": ("15-min Peak Rainfall Intensity (mm/hour)", "peak_intensity_15", "i15"),
    "intensity_30": ("30-min Peak Rainfall Intensity (mm/hour)", "peak_intensity_30", "i30"),
    "duration": ("dur", "storm_duration_hours", "storm_duration"),
}


@dataclass(frozen=True)
class MeasureSpec:
    measure_id: str
    label: str
    value_column: str
    units: str
    optional: bool = False


MEASURE_SPECS: Tuple[MeasureSpec, ...] = (
    MeasureSpec("precip_depth", "Precipitation Depth", "precip_mm", "mm"),
    MeasureSpec("runoff_depth", "Runoff", "runoff_depth_mm", "mm"),
    MeasureSpec("peak_discharge", "Peak Discharge", "peak_discharge_m3s", "m^3/s"),
    MeasureSpec("sediment_yield", "Sediment Yield", "sediment_yield_tonnes", "tonne"),
    MeasureSpec("soluble_reactive_p", "Soluble Reactive P", "soluble_reactive_p_kg", "kg", optional=True),
    MeasureSpec("particulate_p", "Particulate P", "particulate_p_kg", "kg", optional=True),
    MeasureSpec("total_p", "Total P", "total_p_kg", "kg", optional=True),
    MeasureSpec("intensity_10min", "10-min Peak Rainfall Intensity", "intensity_10min", "mm/hour", optional=True),
    MeasureSpec("intensity_15min", "15-min Peak Rainfall Intensity", "intensity_15min", "mm/hour", optional=True),
    MeasureSpec("intensity_30min", "30-min Peak Rainfall Intensity", "intensity_30min", "mm/hour", optional=True),
    MeasureSpec("storm_duration", "Storm Duration", "storm_duration_hours", "hours", optional=True),
    MeasureSpec("hill_sediment", "Hill Sed Del", "hill_sediment_tonnes", "tonne", optional=True),
    MeasureSpec("hill_streamflow", "Hill Streamflow", "hill_streamflow_mm", "mm", optional=True),
)

MEASURE_LOOKUP: Dict[str, MeasureSpec] = {spec.measure_id: spec for spec in MEASURE_SPECS}


def _ensure_path(value: str | Path) -> Path:
    path = Path(value).expanduser().resolve()
    if not path.exists():
        raise FileNotFoundError(path)
    return path


def _quote_identifier(identifier: str) -> str:
    escaped = identifier.replace('"', '""')
    return f'"{escaped}"'


def _discover_climate_asset(base: Path) -> tuple[Path, Dict[str, str]] | tuple[None, Dict[str, str]]:
    climate_dir = base / "climate"
    if not climate_dir.exists():
        return None, {}

    for candidate in sorted(climate_dir.rglob("*.parquet")):
        try:
            schema = pq.read_schema(candidate)
        except Exception:  # pragma: no cover - defensive
            continue
        columns = {field.name for field in schema}

        mapping: Dict[str, str] = {}
        for key, options in CLIMATE_REQUIRED.items():
            selected = next((name for name in options if name in columns), None)
            if selected:
                mapping[key] = selected

        if {"year", "month", "day"}.issubset(mapping):
            return candidate, mapping

    return None, {}


def _build_topaz_filter(topaz_ids: Optional[Sequence[int]]) -> str:
    if not topaz_ids:
        return ""
    values = sorted({int(v) for v in topaz_ids})
    return f"WHERE e.element_id IN ({', '.join(str(v) for v in values)})"


def _build_raw_event_query(
    ebe_path: Path,
    tot_path: Path,
    climate_info: tuple[Path | None, Dict[str, str]],
    topaz_filter: str,
) -> str:
    climate_path, columns = climate_info

    if climate_path is None or not columns:
        climate_join = ""
        intensity_10_expr = "CAST(NULL AS DOUBLE)"
        intensity_15_expr = "CAST(NULL AS DOUBLE)"
        intensity_30_expr = "CAST(NULL AS DOUBLE)"
        duration_expr = "CAST(NULL AS DOUBLE)"
    else:
        climate_alias = "climate"
        year_col = _quote_identifier(columns["year"])
        month_col = _quote_identifier(columns["month"])
        day_col = _quote_identifier(columns["day"])

        def _expr(key: str) -> str:
            column = columns.get(key)
            if column is None:
                return "CAST(NULL AS DOUBLE)"
            return f"{climate_alias}.{_quote_identifier(column)}"

        climate_join = f"""
        LEFT JOIN read_parquet('{climate_path.as_posix()}') AS {climate_alias}
          ON {climate_alias}.{year_col} = e.year
         AND {climate_alias}.{month_col} = e.month
         AND {climate_alias}.{day_col} = e.day_of_month
        """
        intensity_10_expr = _expr("intensity_10")
        intensity_15_expr = _expr("intensity_15")
        intensity_30_expr = _expr("intensity_30")
        duration_expr = _expr("duration")

    return f"""
        SELECT
            ROW_NUMBER() OVER () AS event_id,
            e.element_id AS topaz_id,
            e.simulation_year AS year,
            e.year AS calendar_year,
            e.month AS mo,
            e.day_of_month AS da,
            e.julian,
            e.water_year,
            e.precip AS precip_mm,
            e.runoff_volume AS runoff_volume_m3,
            CASE
                WHEN tot.Area IS NOT NULL AND tot.Area > 0
                    THEN (e.runoff_volume / tot.Area) * 1000.0
                ELSE NULL
            END AS runoff_depth_mm,
            e.peak_runoff AS peak_discharge_m3s,
            e.sediment_yield AS sediment_yield_kg,
            CASE WHEN e.sediment_yield IS NOT NULL THEN e.sediment_yield / 1000.0 ELSE NULL END AS sediment_yield_tonnes,
            e.soluble_pollutant AS soluble_reactive_p_kg,
            e.particulate_pollutant AS particulate_p_kg,
            e.total_pollutant AS total_p_kg,
            tot.Area AS contributing_area_m2,
            tot.tdet AS hill_sediment_kg,
            CASE WHEN tot.tdet IS NOT NULL THEN tot.tdet / 1000.0 ELSE NULL END AS hill_sediment_tonnes,
            tot."Streamflow" AS hill_streamflow_mm,
            {intensity_10_expr} AS intensity_10min,
            {intensity_15_expr} AS intensity_15min,
            {intensity_30_expr} AS intensity_30min,
            {duration_expr} AS storm_duration_hours
        FROM read_parquet('{ebe_path.as_posix()}') AS e
        LEFT JOIN read_parquet('{tot_path.as_posix()}') AS tot
          ON tot.year = e.year
         AND tot.month = e.month
         AND tot.day_of_month = e.day_of_month
        {climate_join}
        {topaz_filter}
    """


def _compute_event_counts(
    connection: duckdb.DuckDBPyConnection,
    ebe_path: Path,
    topaz_filter: str,
) -> pd.DataFrame:
    query = f"""
        SELECT
            e.element_id AS topaz_id,
            e.simulation_year AS year,
            e.month AS month,
            COUNT(*) AS event_count
        FROM read_parquet('{ebe_path.as_posix()}') AS e
        {topaz_filter}
        GROUP BY 1, 2, 3
        ORDER BY 1, 2, 3
    """
    return connection.execute(query).df()


def _build_measure_union(select_alias: str = "event_data") -> str:
    statements: list[str] = []
    for spec in MEASURE_SPECS:
        statements.append(
            f"""
            SELECT
                '{spec.measure_id}' AS measure_id,
                topaz_id,
                event_id,
                {spec.value_column} AS measure_value
            FROM {select_alias}
            """
        )
    return "\nUNION ALL\n".join(statements)


def _serialize_metadata(mapping: Mapping[str, Any]) -> Dict[bytes, bytes]:
    payload: Dict[bytes, bytes] = {}
    for key, value in mapping.items():
        payload[key.encode("utf-8")] = json.dumps(value, separators=(",", ":")).encode("utf-8")
    return payload


def refresh_return_period_events(
    wd: str | Path,
    *,
    topaz_ids: Optional[Sequence[int]] = None,
    max_rank: int = 128,
    buffer: int = 10,
) -> tuple[Path, Path]:
    """
    Regenerate staged return-period assets by querying the interchange Parquet datasets.
    """

    base = _ensure_path(wd)
    activate_query_engine(base, run_interchange=False)
    context = resolve_run_context(str(base), auto_activate=False)
    root_dir = context.base_dir

    ebe_path = root_dir / EBE_DATASET
    if not ebe_path.exists():
        alt_ebe = root_dir / "wepp" / "output" / "ebe_pw0.parquet"
        if alt_ebe.exists():
            ebe_path = alt_ebe
        else:
            raise FileNotFoundError(ebe_path)

    tot_path = root_dir / TOTALWATSED3_DATASET
    if not tot_path.exists():
        raise FileNotFoundError(tot_path)

    climate_info = _discover_climate_asset(root_dir)
    topaz_filter = _build_topaz_filter(topaz_ids)

    events_path = root_dir / EVENTS_REL_PATH
    ranks_path = root_dir / RANKS_REL_PATH
    events_path.parent.mkdir(parents=True, exist_ok=True)

    rank_limit = max(max_rank + buffer, max_rank)

    with duckdb.connect() as con:
        con.execute("PRAGMA threads=4")

        raw_event_query = _build_raw_event_query(ebe_path, tot_path, climate_info, topaz_filter)
        con.execute(f"CREATE OR REPLACE TEMP TABLE raw_events AS {raw_event_query}")

        counts_df = _compute_event_counts(con, ebe_path, topaz_filter)

        measure_union = _build_measure_union("raw_events")
        con.execute(
            f"""
            CREATE OR REPLACE TEMP TABLE ranked_events AS
            SELECT
                measure_id,
                topaz_id,
                event_id,
                measure_value,
                DENSE_RANK() OVER (
                    PARTITION BY measure_id, topaz_id
                    ORDER BY measure_value DESC NULLS LAST
                ) AS rank
            FROM (
                {measure_union}
            )
            WHERE measure_value IS NOT NULL
            """
        )

        con.execute(
            f"""
            CREATE OR REPLACE TEMP TABLE return_period_events AS
            SELECT *
            FROM raw_events
            WHERE event_id IN (
                SELECT DISTINCT event_id
                FROM ranked_events
                WHERE rank <= {rank_limit}
            )
            """
        )

        event_table_reader = con.execute(
            """
            SELECT
                event_id,
                topaz_id,
                year,
                calendar_year,
                mo,
                da,
                julian,
                water_year,
                precip_mm,
                runoff_volume_m3,
                runoff_depth_mm,
                peak_discharge_m3s,
                sediment_yield_kg,
                sediment_yield_tonnes,
                soluble_reactive_p_kg,
                particulate_p_kg,
                total_p_kg,
                contributing_area_m2,
                hill_sediment_kg,
                hill_sediment_tonnes,
                hill_streamflow_mm,
                intensity_10min,
                intensity_15min,
                intensity_30min,
                storm_duration_hours
            FROM return_period_events
            ORDER BY topaz_id, event_id
            """
        ).arrow()
        event_table = event_table_reader.read_all()

        ranks_table_reader = con.execute(
            f"""
            SELECT
                measure_id,
                topaz_id,
                event_id,
                rank,
                measure_value
            FROM ranked_events
            WHERE rank <= {rank_limit}
            ORDER BY measure_id, topaz_id, rank
            """
        ).arrow()
        ranks_table = ranks_table_reader.read_all()

        wsarea_lookup = {}
        if event_table.num_rows > 0:
            tmp_df = event_table.select(["topaz_id", "contributing_area_m2"]).to_pandas()
            tmp_df = tmp_df.dropna(subset=["contributing_area_m2"])
            for topaz_id, group in tmp_df.groupby("topaz_id"):
                wsarea_lookup[str(int(topaz_id))] = float(group["contributing_area_m2"].max() / 10000.0)

        counts_payload = counts_df.to_dict(orient="records")
        measure_metadata = [
            {
                "measure_id": spec.measure_id,
                "label": spec.label,
                "units": spec.units,
                "optional": spec.optional,
                "value_column": spec.value_column,
            }
            for spec in MEASURE_SPECS
        ]

        events_metadata = _serialize_metadata(
            {
                "watershed_area_ha": wsarea_lookup,
                "event_schema_version": 1,
            }
        )
        ranks_metadata = _serialize_metadata(
            {
                "measure_metadata": measure_metadata,
                "event_counts": counts_payload,
                "rank_limit": rank_limit,
            }
        )

        event_table = event_table.combine_chunks().replace_schema_metadata(events_metadata)
        ranks_table = ranks_table.combine_chunks().replace_schema_metadata(ranks_metadata)

        pq.write_table(event_table, events_path, compression="snappy")
        pq.write_table(ranks_table, ranks_path, compression="snappy")

    try:
        update_catalog_entry(root_dir, str(EVENTS_REL_PATH))
    except FileNotFoundError:  # pragma: no cover - catalog may not exist yet
        activate_query_engine(root_dir, run_interchange=False)

    try:
        update_catalog_entry(root_dir, str(RANKS_REL_PATH))
    except FileNotFoundError:  # pragma: no cover
        activate_query_engine(root_dir, run_interchange=False)

    return events_path, ranks_path


class ReturnPeriodDataset:
    def __init__(
        self,
        wd: str | Path,
        *,
        auto_refresh: bool = True,
        max_rank: int = 128,
        buffer: int = 10,
    ) -> None:
        base = _ensure_path(wd)
        context = resolve_run_context(str(base), auto_activate=False)
        self._root = context.base_dir

        self._events_path = self._root / EVENTS_REL_PATH
        self._ranks_path = self._root / RANKS_REL_PATH

        if auto_refresh and (not self._events_path.exists() or not self._ranks_path.exists()):
            refresh_return_period_events(base, max_rank=max_rank, buffer=buffer)

        self._events_table = pq.read_table(self._events_path)
        self._ranks_table = pq.read_table(self._ranks_path)

        self._events = self._events_table.to_pandas()
        self._ranks = self._ranks_table.to_pandas()

        metadata_events = self._events_table.schema.metadata or {}
        metadata_ranks = self._ranks_table.schema.metadata or {}

        if b"watershed_area_ha" in metadata_events:
            wsarea_payload = json.loads(metadata_events[b"watershed_area_ha"].decode("utf-8"))
            self._watershed_area = {str(key): float(value) for key, value in wsarea_payload.items()}
        else:
            self._watershed_area = {}

        self._event_counts = pd.DataFrame(
            json.loads(metadata_ranks.get(b"event_counts", b"[]").decode("utf-8"))
        ) if b"event_counts" in metadata_ranks else pd.DataFrame(columns=["topaz_id", "year", "month", "event_count"])

        measure_payload = json.loads(metadata_ranks.get(b"measure_metadata", b"[]").decode("utf-8")) if b"measure_metadata" in metadata_ranks else []
        self._measure_specs: Dict[str, MeasureSpec] = {}
        for entry in measure_payload:
            spec = MeasureSpec(
                entry["measure_id"],
                entry["label"],
                entry["value_column"],
                entry["units"],
                entry.get("optional", False),
            )
            self._measure_specs[spec.measure_id] = spec

        if not self._measure_specs:
            self._measure_specs = MEASURE_LOOKUP.copy()

        self._has_phosphorus = any(
            col in self._events.columns and not self._events[col].isna().all()
            for col in ("soluble_reactive_p_kg", "particulate_p_kg", "total_p_kg")
        )

    @property
    def topaz_ids(self) -> list[int]:
        if "topaz_id" not in self._events.columns:
            return []
        ids = sorted(set(int(v) for v in self._events["topaz_id"].unique()))
        return ids

    def _events_for_topaz(self, topaz_id: int) -> pd.DataFrame:
        return self._events[self._events["topaz_id"] == topaz_id].copy()

    def _ranks_for_topaz(self, topaz_id: int) -> pd.DataFrame:
        return self._ranks[self._ranks["topaz_id"] == topaz_id].copy()

    def _counts_for_topaz(self, topaz_id: int) -> pd.DataFrame:
        if self._event_counts.empty:
            return pd.DataFrame(columns=["year", "month", "event_count"])
        return self._event_counts[self._event_counts["topaz_id"] == topaz_id].copy()

    def create_report(
        self,
        recurrence: Sequence[int],
        *,
        exclude_yr_indxs: Optional[Sequence[int]] = None,
        exclude_months: Optional[Sequence[int]] = None,
        method: str = "cta",
        gringorten_correction: bool = False,
        topaz_id: Optional[int] = None,
    ) -> "ReturnPeriods":
        if not recurrence:
            raise ValueError("recurrence intervals must be provided")

        available = self.topaz_ids
        if not available:
            raise ValueError("No channel events are available in the staged dataset")
        target_topaz = topaz_id if topaz_id is not None else available[0]
        if target_topaz not in available:
            raise ValueError(f"Topaz ID {target_topaz} not present in staged events")

        events = self._events_for_topaz(target_topaz)
        ranks = self._ranks_for_topaz(target_topaz)
        counts = self._counts_for_topaz(target_topaz)

        if events.empty:
            raise ValueError(f"No ranked events available after staging for Topaz {target_topaz}")

        exclude_months = tuple(sorted(set(int(m) for m in exclude_months))) if exclude_months else ()

        if exclude_months:
            events = events[~events["mo"].isin(exclude_months)]
            ranks = ranks[ranks["event_id"].isin(events["event_id"])]

        year_values = sorted(int(y) for y in events["year"].dropna().unique())
        if not year_values:
            raise ValueError("Unable to determine simulation years for events")

        exclude_yr_indxs = tuple(sorted(set(int(i) for i in exclude_yr_indxs))) if exclude_yr_indxs else ()
        excluded_years = {year_values[idx] for idx in exclude_yr_indxs if 0 <= idx < len(year_values)}
        if excluded_years:
            events = events[~events["year"].isin(excluded_years)]
            ranks = ranks[ranks["event_id"].isin(events["event_id"])]

        counts_filtered = counts.copy()
        if not counts_filtered.empty:
            if exclude_months:
                counts_filtered = counts_filtered[~counts_filtered["month"].isin(exclude_months)]
            if excluded_years:
                counts_filtered = counts_filtered[~counts_filtered["year"].isin(excluded_years)]

        if counts_filtered.empty:
            total_events = len(events)
            years_count = len(set(events["year"]))
        else:
            total_events = int(counts_filtered["event_count"].sum())
            years_count = len(set(counts_filtered["year"]))

        years_count = max(years_count, 1)
        days_in_year = total_events / years_count if years_count > 0 else 0.0

        rec_map = weibull_series(recurrence, years_count, method=method, gringorten_correction=gringorten_correction)

        measure_results: Dict[str, Dict[int, Dict[str, float]]] = {}

        seen_specs: set[str] = set()
        ordered_specs: list[MeasureSpec] = []
        for template_spec in MEASURE_SPECS:
            resolved = self._measure_specs.get(template_spec.measure_id, template_spec)
            ordered_specs.append(resolved)
            seen_specs.add(resolved.measure_id)
        for measure_id, spec in self._measure_specs.items():
            if measure_id not in seen_specs:
                ordered_specs.append(spec)
                seen_specs.add(measure_id)

        for spec in ordered_specs:
            if spec.measure_id not in ranks["measure_id"].values:
                continue

            subset = ranks[ranks["measure_id"] == spec.measure_id]
            if subset.empty:
                continue

            merged = subset.merge(events, on="event_id", how="inner", suffixes=("", "_event"))
            if merged.empty:
                continue

            merged = merged.sort_values(["measure_value", "event_id"], ascending=[False, True]).reset_index(drop=True)

            if method.lower() != "cta":
                merged = (
                    merged.sort_values(["year", "measure_value"], ascending=[True, False])
                    .groupby("year", as_index=False, sort=False)
                    .head(1)
                )
                merged = merged.sort_values(["measure_value", "event_id"], ascending=[False, True]).reset_index(drop=True)

            merged["weibull_rank"] = merged.index + 1
            if days_in_year > 0:
                merged["weibull_T"] = ((total_events + 1) / merged["weibull_rank"]) / days_in_year
            else:
                merged["weibull_T"] = np.nan

            display_label = spec.label
            rows_for_measure: Dict[int, Dict[str, float]] = {}

            for ret, idx in rec_map.items():
                if idx >= len(merged):
                    continue
                row = merged.iloc[idx]
                formatted = _format_event_row(row, spec)
                rows_for_measure[int(ret)] = formatted

            if rows_for_measure:
                measure_results[display_label] = rows_for_measure

        wsarea = self._watershed_area.get(str(target_topaz))
        wsarea_val = float(wsarea) if wsarea is not None else np.nan

        report = ReturnPeriods.__new__(ReturnPeriods)  # bypass __init__
        report.has_phosphorus = self._has_phosphorus
        report.header = [
            "Precipitation Depth (mm)",
            "Runoff Volume (m^3)",
            "Peak Runoff (m^3/s)",
            "Sediment Yield (kg)",
            "Soluble Reactive P (kg)",
            "Particulate P (kg)",
            "Total P (kg)",
            "TopazID",
            "Hill Sed Del (kg)",
            "10-min Peak Rainfall Intensity (mm/hour)",
            "15-min Peak Rainfall Intensity (mm/hour)",
            "30-min Peak Rainfall Intensity (mm/hour)",
            "Storm Duration (hours)",
            "Hill Streamflow (mm)",
        ]
        report.method = method
        report.gringorten_correction = gringorten_correction
        report.y0 = year_values[0] if year_values else 0
        report.years = years_count
        report.wsarea = wsarea_val
        report.recurrence = sorted(int(r) for r in recurrence)
        report.return_periods = measure_results
        report.num_events = total_events
        report.intervals = sorted(measure_results[next(iter(measure_results))].keys()) if measure_results else []
        report.units_d = {
            "Precipitation Depth": "mm",
            "Runoff": "mm",
            "Peak Discharge": "m^3/s",
            "Sediment Yield": "tonne",
            "Soluble Reactive P": "kg",
            "Particulate P": "kg",
            "Total P": "kg",
            "10-min Peak Rainfall Intensity": "mm/hour",
            "15-min Peak Rainfall Intensity": "mm/hour",
            "30-min Peak Rainfall Intensity": "mm/hour",
            "Storm Duration": "hours",
            "Hill Sed Del": "tonne",
            "Hill Streamflow": "mm",
        }
        report.exclude_yr_indxs = list(exclude_yr_indxs) if exclude_yr_indxs else None
        report.exclude_months = list(exclude_months) if exclude_months else None
        return report


def _format_event_row(row: pd.Series, spec: MeasureSpec) -> Dict[str, Any]:
    result: Dict[str, Any] = {
        "mo": int(row["mo"]),
        "da": int(row["da"]),
        "year": int(row["year"]),
        "TopazID": int(row["topaz_id"]),
        "Precipitation Depth": float(row.get("precip_mm", np.nan)),
        "Runoff": float(row.get("runoff_depth_mm", np.nan)) if not pd.isna(row.get("runoff_depth_mm")) else 0.0,
        "Peak Discharge": float(row.get("peak_discharge_m3s", np.nan)),
        "Sediment Yield": float(row.get("sediment_yield_tonnes", np.nan)),
        "Soluble Reactive P": float(row.get("soluble_reactive_p_kg", np.nan))
        if "soluble_reactive_p_kg" in row else np.nan,
        "Particulate P": float(row.get("particulate_p_kg", np.nan)) if "particulate_p_kg" in row else np.nan,
        "Total P": float(row.get("total_p_kg", np.nan)) if "total_p_kg" in row else np.nan,
        "Hill Sed Del": float(row.get("hill_sediment_tonnes", np.nan)),
        "Hill Streamflow": float(row.get("hill_streamflow_mm", np.nan)),
        "10-min Peak Rainfall Intensity": float(row.get("intensity_10min", np.nan)),
        "15-min Peak Rainfall Intensity": float(row.get("intensity_15min", np.nan)),
        "30-min Peak Rainfall Intensity": float(row.get("intensity_30min", np.nan)),
        "Storm Duration": float(row.get("storm_duration_hours", np.nan)),
        "weibull_rank": int(row["weibull_rank"]),
        "weibull_T": float(row.get("weibull_T", np.nan)),
    }

    target_value = row.get(spec.value_column)
    if target_value is not None and not pd.isna(target_value):
        result[spec.label] = float(target_value)

    return result


class ReturnPeriods:
    def __init__(self, *args, **kwargs):
        raise RuntimeError(
            "ReturnPeriods must be constructed using ReturnPeriodDataset.create_report; "
            "direct construction is no longer supported."
        )

    def to_dict(self) -> Dict[str, Any]:
        return {
            "has_phosphorus": self.has_phosphorus,
            "header": self.header,
            "method": self.method,
            "gringorten_correction": self.gringorten_correction,
            "y0": self.y0,
            "years": self.years,
            "wsarea": self.wsarea,
            "recurrence": self.recurrence,
            "return_periods": self.return_periods,
            "num_events": self.num_events,
            "intervals": self.intervals,
            "units_d": self.units_d,
            "exclude_yr_indxs": self.exclude_yr_indxs,
            "exclude_months": self.exclude_months,
        }

    @classmethod
    def from_dict(cls, data: Mapping[str, Any]) -> "ReturnPeriods":
        instance = cls.__new__(cls)
        instance.has_phosphorus = data.get("has_phosphorus", False)
        instance.header = list(data.get("header", []))
        instance.method = data.get("method", "cta")
        instance.gringorten_correction = data.get("gringorten_correction", False)
        instance.y0 = data.get("y0", 0)
        instance.years = data.get("years", 0)
        instance.wsarea = data.get("wsarea")
        instance.recurrence = list(data.get("recurrence", []))
        instance.return_periods = {}
        ret_periods = data.get("return_periods", {})
        for measure, mapping in ret_periods.items():
            instance.return_periods[measure] = {}
            for key, value in mapping.items():
                instance.return_periods[measure][int(key)] = value
        instance.num_events = data.get("num_events", 0)
        instance.intervals = list(data.get("intervals", []))
        instance.units_d = dict(data.get("units_d", {}))
        instance.exclude_yr_indxs = data.get("exclude_yr_indxs")
        instance.exclude_months = data.get("exclude_months")
        return instance

    def export_tsv_summary(self, summary_path: str | Path, extraneous: bool = False) -> None:
        summary_path = Path(summary_path)
        if extraneous:
            self._export_tsv_summary_extraneous(summary_path)
        else:
            self._export_tsv_summary_simple(summary_path)

    def _export_tsv_summary_simple(self, summary_path: Path) -> None:
        measures = [
            "Precipitation Depth",
            "Runoff",
            "Peak Discharge",
            "10-min Peak Rainfall Intensity",
            "15-min Peak Rainfall Intensity",
            "30-min Peak Rainfall Intensity",
            "Sediment Yield",
        ]
        if self.has_phosphorus:
            measures.extend(["Soluble Reactive P", "Particulate P", "Total P"])

        with summary_path.open("w", encoding="utf-8") as stream:
            stream.write("WEPPcloud Return Period Analysis\n")
            stream.write(f"Years in Simulation\t{self.years}\n")
            stream.write(f"Events in Simulation\t{self.num_events}\n")
            if self.exclude_yr_indxs:
                stream.write(f"Excluded Year Indexes\t{', '.join(map(str, self.exclude_yr_indxs))}\n")
            if self.gringorten_correction:
                stream.write("Using Gringorten Correction for Weibull formula\n")
            stream.write("\n")

            for key in measures:
                if key not in self.return_periods:
                    continue
                stream.write(f"{key}\n")
                unit = self.units_d.get(key, "")
                stream.write("Recurrence Interval (years)\tDate (mm/dd/yyyy)\t")
                stream.write(f"{key} ({unit})\n")
                for rec_interval in sorted(self.return_periods[key], reverse=True):
                    row = self.return_periods[key][rec_interval]
                    date = f"{int(row['mo']):02d}/{int(row['da']):02d}/{int(row['year'] + self.y0 - 1):04d}"
                    value = row.get(key, 0.0)
                    stream.write(f"{rec_interval}\t{date}\t{value:.2f}\n")
                stream.write("\n")

    def _export_tsv_summary_extraneous(self, summary_path: Path) -> None:
        measures = [
            "Precipitation Depth",
            "Runoff",
            "Peak Discharge",
            "10-min Peak Rainfall Intensity",
            "15-min Peak Rainfall Intensity",
            "30-min Peak Rainfall Intensity",
            "Sediment Yield",
        ]
        if self.has_phosphorus:
            measures.extend(["Soluble Reactive P", "Particulate P", "Total P"])

        with summary_path.open("w", encoding="utf-8") as stream:
            stream.write("WEPPcloud Return Period Analysis\n")
            stream.write(f"Years in Simulation\t{self.years}\n")
            stream.write(f"Events in Simulation\t{self.num_events}\n")
            if self.exclude_yr_indxs:
                stream.write(f"Excluded Year Indexes\t{', '.join(map(str, self.exclude_yr_indxs))}\n")
            else:
                stream.write("Excluded Year Indexes\tNone\n")
            stream.write("\n")

            for key in measures:
                if key not in self.return_periods:
                    continue

                headers = [
                    "Recurrence Interval (years)",
                    "Date (mm/dd/yyyy)",
                    f"Precipitation Depth ({self.units_d.get('Precipitation Depth', 'mm')})",
                    f"Runoff ({self.units_d.get('Runoff', 'mm')})",
                    f"Peak Discharge ({self.units_d.get('Peak Discharge', 'm^3/s')})",
                ]
                if "10-min Peak Rainfall Intensity" in self.return_periods:
                    headers.append(f"10-min Peak Rainfall Intensity ({self.units_d.get('10-min Peak Rainfall Intensity', 'mm/hour')})")
                if "15-min Peak Rainfall Intensity" in self.return_periods:
                    headers.append(f"15-min Peak Rainfall Intensity ({self.units_d.get('15-min Peak Rainfall Intensity', 'mm/hour')})")
                if "30-min Peak Rainfall Intensity" in self.return_periods:
                    headers.append(f"30-min Peak Rainfall Intensity ({self.units_d.get('30-min Peak Rainfall Intensity', 'mm/hour')})")
                if "Storm Duration" in self.return_periods:
                    headers.append(f"Storm Duration ({self.units_d.get('Storm Duration', 'hours')})")
                headers.append(f"Sediment Yield ({self.units_d.get('Sediment Yield', 'tonne')})")
                if self.has_phosphorus:
                    headers.extend(
                        [
                            f"Soluble Reactive P ({self.units_d.get('Soluble Reactive P', 'kg')})",
                            f"Particulate P ({self.units_d.get('Particulate P', 'kg')})",
                            f"Total P ({self.units_d.get('Total P', 'kg')})",
                        ]
                    )
                headers.extend(["Rank", "Weibull T"])
                stream.write(f"{key}\n")
                stream.write("\t".join(headers) + "\n")

                for rec_interval in sorted(self.return_periods[key], reverse=True):
                    row = self.return_periods[key][rec_interval]
                    date = f"{int(row['mo']):02d}/{int(row['da']):02d}/{int(row['year'] + self.y0 - 1):04d}"
                    values = [
                        str(rec_interval),
                        date,
                        f"{row.get('Precipitation Depth', 0):.2f}",
                        f"{row.get('Runoff', 0):.2f}",
                        f"{row.get('Peak Discharge', 0):.2f}",
                    ]
                    if "10-min Peak Rainfall Intensity" in self.return_periods:
                        values.append(f"{row.get('10-min Peak Rainfall Intensity', 0):.2f}")
                    if "15-min Peak Rainfall Intensity" in self.return_periods:
                        values.append(f"{row.get('15-min Peak Rainfall Intensity', 0):.2f}")
                    if "30-min Peak Rainfall Intensity" in self.return_periods:
                        values.append(f"{row.get('30-min Peak Rainfall Intensity', 0):.2f}")
                    if "Storm Duration" in self.return_periods:
                        values.append(f"{row.get('Storm Duration', 0):.2f}")
                    values.append(f"{row.get('Sediment Yield', 0):.2f}")
                    if self.has_phosphorus:
                        values.extend(
                            [
                                f"{row.get('Soluble Reactive P', 0):.2f}",
                                f"{row.get('Particulate P', 0):.2f}",
                                f"{row.get('Total P', 0):.2f}",
                            ]
                        )
                    values.extend(
                        [
                            str(row.get("weibull_rank", "")),
                            f"{row.get('weibull_T', 0):.2f}",
                        ]
                    )
                    stream.write("\t".join(values) + "\n")
                stream.write("\n")


__all__ = [
    "ReturnPeriodDataset",
    "ReturnPeriods",
    "refresh_return_period_events",
]
